{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===============================\n# MEGA TEMPLATE: BINARY CLASSIFICATION MODELS (KAGGLE / EXAM READY)\n# Covers:\n# Logistic Regression\n# Decision Tree\n# Random Forest\n# Gradient Boosting\n# XGBoost\n# Support Vector Machine (SVM)\n# K-Nearest Neighbors (KNN)\n# Naive Bayes (GaussianNB)\n# ===============================\n\n# -------------------------------\n# 1. IMPORT LIBRARIES\n# -------------------------------\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n# Optional (Kaggle favourite)\nfrom xgboost import XGBClassifier\n\n# -------------------------------\n# 2. LOAD DATA\n# -------------------------------\n# Example: df = pd.read_csv(\"train.csv\")\n\n# Assume:\n# - target column name = 'target'\n# - binary labels: 0 and 1\n\nX = df.drop('target', axis=1)\ny = df['target']\n\n# -------------------------------\n# 3. HANDLE CATEGORICAL FEATURES\n# -------------------------------\n# Label Encoding (simple & exam-friendly)\nfor col in X.select_dtypes(include='object').columns:\n    X[col] = LabelEncoder().fit_transform(X[col])\n\n# -------------------------------\n# 4. TRAIN-TEST SPLIT\n# -------------------------------\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.2,\n    random_state=42,\n    stratify=y\n)\n\n# -------------------------------\n# 5. FEATURE SCALING (Required for some models)\n# -------------------------------\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n\n\n\n\n\n# =====================================================\n# MODEL 1: LOGISTIC REGRESSION\n# =====================================================\nlog_reg = LogisticRegression(\n    max_iter=1000,\n    penalty='l2',\n    class_weight='balanced'\n)\nlog_reg.fit(X_train_scaled, y_train)\n\ny_pred_lr = log_reg.predict(X_test_scaled)\ny_prob_lr = log_reg.predict_proba(X_test_scaled)[:, 1]\n\nprint(\"Logistic Regression\")\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred_lr))\nprint(\"ROC-AUC:\", roc_auc_score(y_test, y_prob_lr))\nprint(classification_report(y_test, y_pred_lr))\n\n\n\n\n\n# =====================================================\n# MODEL 2: DECISION TREE\n# =====================================================\ndt = DecisionTreeClassifier(\n    criterion='gini',\n    max_depth=6,\n    min_samples_split=10\n)\ndt.fit(X_train, y_train)\n\ny_pred_dt = dt.predict(X_test)\n\nprint(\"Decision Tree\")\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred_dt))\nprint(classification_report(y_test, y_pred_dt))\n\n\n\n\n\n\n# =====================================================\n# MODEL 3: RANDOM FOREST\n# =====================================================\nrf = RandomForestClassifier(\n    n_estimators=300,\n    max_depth=12,\n    n_jobs=-1,\n    random_state=42\n)\nrf.fit(X_train, y_train)\n\ny_pred_rf = rf.predict(X_test)\ny_prob_rf = rf.predict_proba(X_test)[:, 1]\n\nprint(\"Random Forest\")\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred_rf))\nprint(\"ROC-AUC:\", roc_auc_score(y_test, y_prob_rf))\n\n\n\n\n\n\n# =====================================================\n# MODEL 4: GRADIENT BOOSTING\n# =====================================================\ngb = GradientBoostingClassifier(\n    n_estimators=200,\n    learning_rate=0.05,\n    max_depth=3\n)\ngb.fit(X_train, y_train)\n\ny_pred_gb = gb.predict(X_test)\ny_prob_gb = gb.predict_proba(X_test)[:, 1]\n\nprint(\"Gradient Boosting\")\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred_gb))\nprint(\"ROC-AUC:\", roc_auc_score(y_test, y_prob_gb))\n\n\n\n\n\n\n# =====================================================\n# MODEL 5: XGBOOST (STATE-OF-THE-ART)\n# =====================================================\nxgb = XGBClassifier(\n    n_estimators=500,\n    max_depth=6,\n    learning_rate=0.05,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    eval_metric='logloss',\n    random_state=42\n)\nxgb.fit(X_train, y_train)\n\ny_pred_xgb = xgb.predict(X_test)\ny_prob_xgb = xgb.predict_proba(X_test)[:, 1]\n\nprint(\"XGBoost\")\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred_xgb))\nprint(\"ROC-AUC:\", roc_auc_score(y_test, y_prob_xgb))\n\n\n\n\n\n\n\n# =====================================================\n# MODEL 6: SUPPORT VECTOR MACHINE (SVM)\n# =====================================================\nsvm = SVC(\n    kernel='rbf',\n    C=1.0,\n    gamma='scale',\n    probability=True\n)\nsvm.fit(X_train_scaled, y_train)\n\ny_pred_svm = svm.predict(X_test_scaled)\ny_prob_svm = svm.predict_proba(X_test_scaled)[:, 1]\n\nprint(\"SVM\")\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred_svm))\nprint(\"ROC-AUC:\", roc_auc_score(y_test, y_prob_svm))\n\n\n\n\n\n\n# =====================================================\n# MODEL 7: K-NEAREST NEIGHBORS\n# =====================================================\nknn = KNeighborsClassifier(\n    n_neighbors=7,\n    weights='distance'\n)\nknn.fit(X_train_scaled, y_train)\n\ny_pred_knn = knn.predict(X_test_scaled)\n\nprint(\"KNN\")\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred_knn))\n\n\n\n\n\n\n# =====================================================\n# MODEL 8: NAIVE BAYES\n# =====================================================\nnb = GaussianNB()\nnb.fit(X_train_scaled, y_train)\n\ny_pred_nb = nb.predict(X_test_scaled)\n\nprint(\"Naive Bayes\")\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred_nb))\nprint(classification_report(y_test, y_pred_nb))\n\n# ===============================\n# END OF MEGA BINARY CLASSIFICATION TEMPLATE\n# ===============================\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===============================\n# MEGA TEMPLATE: MULTICLASS CLASSIFICATION MODELS (KAGGLE / EXAM READY)\n# Covers:\n# Logistic Regression (Softmax)\n# Decision Tree\n# Random Forest\n# Gradient Boosting\n# XGBoost (Multiclass)\n# Support Vector Machine (OvR / OvO)\n# K-Nearest Neighbors (KNN)\n# Naive Bayes (GaussianNB)\n# ===============================\n\n# -------------------------------\n# 1. IMPORT LIBRARIES\n# -------------------------------\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.metrics import accuracy_score, classification_report\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n# Optional (Kaggle favourite)\nfrom xgboost import XGBClassifier\n\n# -------------------------------\n# 2. LOAD DATA\n# -------------------------------\n# Example:\n# df = pd.read_csv(\"train.csv\")\n\n# Assume:\n# - target column name = 'target'\n# - target has more than 2 classes\n\nX = df.drop('target', axis=1)\ny = df['target']\n\n# -------------------------------\n# 3. HANDLE CATEGORICAL FEATURES\n# -------------------------------\nfor col in X.select_dtypes(include='object').columns:\n    X[col] = LabelEncoder().fit_transform(X[col])\n\n# Encode target if needed\nif y.dtype == 'object':\n    y = LabelEncoder().fit_transform(y)\n\n# -------------------------------\n# 4. TRAIN-TEST SPLIT\n# -------------------------------\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.2,\n    random_state=42,\n    stratify=y\n)\n\n# -------------------------------\n# 5. FEATURE SCALING (Required for some models)\n# -------------------------------\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# =====================================================\n# MODEL 1: LOGISTIC REGRESSION (SOFTMAX)\n# =====================================================\nlog_reg = LogisticRegression(\n    multi_class='multinomial',\n    solver='lbfgs',\n    max_iter=1000\n)\nlog_reg.fit(X_train_scaled, y_train)\n\ny_pred_lr = log_reg.predict(X_test_scaled)\n\nprint(\"Logistic Regression (Softmax)\")\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred_lr))\nprint(classification_report(y_test, y_pred_lr))\n\n# =====================================================\n# MODEL 2: DECISION TREE\n# =====================================================\ndt = DecisionTreeClassifier(\n    criterion='gini',\n    max_depth=8,\n    min_samples_split=10\n)\ndt.fit(X_train, y_train)\n\ny_pred_dt = dt.predict(X_test)\n\nprint(\"Decision Tree\")\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred_dt))\nprint(classification_report(y_test, y_pred_dt))\n\n# =====================================================\n# MODEL 3: RANDOM FOREST\n# =====================================================\nrf = RandomForestClassifier(\n    n_estimators=300,\n    max_depth=15,\n    n_jobs=-1,\n    random_state=42\n)\nrf.fit(X_train, y_train)\n\ny_pred_rf = rf.predict(X_test)\n\nprint(\"Random Forest\")\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred_rf))\nprint(classification_report(y_test, y_pred_rf))\n\n# =====================================================\n# MODEL 4: GRADIENT BOOSTING\n# =====================================================\ngb = GradientBoostingClassifier(\n    n_estimators=200,\n    learning_rate=0.05,\n    max_depth=3\n)\ngb.fit(X_train, y_train)\n\ny_pred_gb = gb.predict(X_test)\n\nprint(\"Gradient Boosting\")\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred_gb))\nprint(classification_report(y_test, y_pred_gb))\n\n# =====================================================\n# MODEL 5: XGBOOST (MULTICLASS)\n# =====================================================\nnum_classes = len(np.unique(y_train))\n\nxgb = XGBClassifier(\n    objective='multi:softprob',\n    num_class=num_classes,\n    n_estimators=500,\n    max_depth=6,\n    learning_rate=0.05,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    eval_metric='mlogloss',\n    random_state=42\n)\nxgb.fit(X_train, y_train)\n\ny_pred_xgb = xgb.predict(X_test)\n\nprint(\"XGBoost (Multiclass)\")\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred_xgb))\nprint(classification_report(y_test, y_pred_xgb))\n\n# =====================================================\n# MODEL 6: SUPPORT VECTOR MACHINE (OvR)\n# =====================================================\nsvm = SVC(\n    kernel='rbf',\n    C=1.0,\n    gamma='scale'\n)\nsvm.fit(X_train_scaled, y_train)\n\ny_pred_svm = svm.predict(X_test_scaled)\n\nprint(\"SVM (One-vs-Rest)\")\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred_svm))\nprint(classification_report(y_test, y_pred_svm))\n\n# =====================================================\n# MODEL 7: K-NEAREST NEIGHBORS\n# =====================================================\nknn = KNeighborsClassifier(\n    n_neighbors=7,\n    weights='distance'\n)\nknn.fit(X_train_scaled, y_train)\n\ny_pred_knn = knn.predict(X_test_scaled)\n\nprint(\"KNN\")\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred_knn))\nprint(classification_report(y_test, y_pred_knn))\n\n# =====================================================\n# MODEL 8: NAIVE BAYES\n# =====================================================\nnb = GaussianNB()\nnb.fit(X_train_scaled, y_train)\n\ny_pred_nb = nb.predict(X_test_scaled)\n\nprint(\"Naive Bayes\")\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred_nb))\nprint(classification_report(y_test, y_pred_nb))\n\n# ===============================\n# END OF MEGA MULTICLASS CLASSIFICATION TEMPLATE\n# ===============================\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================\n# MEGA TEMPLATE: OUTLIER ANALYSIS (EDA + HANDLING)\n# Kaggle / Exam / Notebook Beautiful Plots\n# ============================================\n\n# -------------------------------\n# 1. IMPORT LIBRARIES\n# -------------------------------\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom scipy import stats\n\n# -------------------------------\n# 2. LOAD DATA\n# -------------------------------\n# Example:\n# df = pd.read_csv('train.csv')\n\n# Select only numerical columns for outlier analysis\nnum_cols = df.select_dtypes(include=['int64', 'float64']).columns\n\n# -------------------------------\n# 3. BASIC STATISTICAL SUMMARY\n# -------------------------------\nprint(\"Statistical Summary:\\n\")\nprint(df[num_cols].describe())\n\n# -------------------------------\n# 4. UNIVARIATE OUTLIER VISUALIZATION\n# -------------------------------\n# 4.1 Boxplots (Most important)\nplt.figure(figsize=(15, 6))\ndf[num_cols].boxplot(rot=90)\nplt.title('Boxplot of Numerical Features')\nplt.tight_layout()\nplt.show()\n\n# 4.2 Histograms + KDE\nfor col in num_cols:\n    plt.figure(figsize=(6, 4))\n    sns.histplot(df[col], kde=True)\n    plt.title(f'Distribution of {col}')\n    plt.show()\n\n# -------------------------------\n# 5. IQR METHOD (CLASSIC EXAM METHOD)\n# -------------------------------\noutlier_summary = {}\n\nfor col in num_cols:\n    Q1 = df[col].quantile(0.25)\n    Q3 = df[col].quantile(0.75)\n    IQR = Q3 - Q1\n    \n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n    \n    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n    outlier_summary[col] = len(outliers)\n\nprint(\"Outlier count using IQR method:\\n\")\nfor k, v in outlier_summary.items():\n    print(f\"{k}: {v}\")\n\n# -------------------------------\n# 6. Z-SCORE METHOD\n# -------------------------------\nzscore_outliers = {}\n\nfor col in num_cols:\n    z_scores = np.abs(stats.zscore(df[col].dropna()))\n    count = np.sum(z_scores > 3)\n    zscore_outliers[col] = count\n\nprint(\"\\nOutlier count using Z-score method:\\n\")\nfor k, v in zscore_outliers.items():\n    print(f\"{k}: {v}\")\n\n# -------------------------------\n# 7. MULTIVARIATE OUTLIERS (SCATTER)\n# -------------------------------\n# Example: first two numerical columns\nif len(num_cols) >= 2:\n    plt.figure(figsize=(6, 5))\n    sns.scatterplot(x=df[num_cols[0]], y=df[num_cols[1]])\n    plt.title('Scatter Plot for Multivariate Outliers')\n    plt.show()\n\n# -------------------------------\n# 8. OUTLIER HANDLING TECHNIQUES\n# -------------------------------\n\n# 8.1 REMOVE OUTLIERS USING IQR\n\ndef remove_outliers_iqr(data, col):\n    Q1 = data[col].quantile(0.25)\n    Q3 = data[col].quantile(0.75)\n    IQR = Q3 - Q1\n    lower = Q1 - 1.5 * IQR\n    upper = Q3 + 1.5 * IQR\n    return data[(data[col] >= lower) & (data[col] <= upper)]\n\n# Apply iteratively (use carefully!)\ndf_iqr_cleaned = df.copy()\nfor col in num_cols:\n    df_iqr_cleaned = remove_outliers_iqr(df_iqr_cleaned, col)\n\nprint(\"Shape before IQR cleaning:\", df.shape)\nprint(\"Shape after IQR cleaning:\", df_iqr_cleaned.shape)\n\n# -------------------------------\n# 8.2 CAPPING (WINSORIZATION)\n# -------------------------------\n\ndef cap_outliers(data, col):\n    Q1 = data[col].quantile(0.25)\n    Q3 = data[col].quantile(0.75)\n    IQR = Q3 - Q1\n    lower = Q1 - 1.5 * IQR\n    upper = Q3 + 1.5 * IQR\n    data[col] = np.where(data[col] < lower, lower, data[col])\n    data[col] = np.where(data[col] > upper, upper, data[col])\n    return data\n\n# Apply capping\ndf_capped = df.copy()\nfor col in num_cols:\n    df_capped = cap_outliers(df_capped, col)\n\n# -------------------------------\n# 8.3 LOG TRANSFORMATION (FOR SKEWED DATA)\n# -------------------------------\nfor col in num_cols:\n    if (df[col] > 0).all():\n        df[f'{col}_log'] = np.log1p(df[col])\n\n# -------------------------------\n# 9. MODEL-SAFE APPROACH (TREE MODELS)\n# -------------------------------\n# NOTE:\n# - Tree-based models (Decision Tree, Random Forest, XGBoost)\n#   are robust to outliers.\n# - Scaling + outlier removal is CRITICAL for:\n#   Logistic Regression, SVM, KNN\n\n# -------------------------------\n# 10. FINAL CHECK (AFTER CLEANING)\n# -------------------------------\nplt.figure(figsize=(15, 6))\ndf_capped[num_cols].boxplot(rot=90)\nplt.title('Boxplot After Outlier Treatment (Capped)')\nplt.tight_layout()\nplt.show()\n\n# ============================================\n# END OF OUTLIER ANALYSIS MEGA TEMPLATE\n# ============================================\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =====================================================\n# MEGA TEMPLATE: EDA PLOTTING (ALL IMPORTANT GRAPHS)\n# Kaggle | Exam | Notebook-Beautiful Visuals\n# =====================================================\n\n# -------------------------------\n# 1. IMPORT LIBRARIES\n# -------------------------------\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# -------------------------------\n# 2. LOAD DATA\n# -------------------------------\n# Example:\n# df = pd.read_csv('train.csv')\n\n# Separate numerical & categorical columns\nnum_cols = df.select_dtypes(include=['int64', 'float64']).columns\ncat_cols = df.select_dtypes(include=['object', 'category']).columns\n\n# -------------------------------\n# 3. UNIVARIATE PLOTS\n# -------------------------------\n\n# 3.1 Histogram\nfor col in num_cols:\n    plt.figure(figsize=(6,4))\n    plt.hist(df[col], bins=30)\n    plt.title(f'Histogram of {col}')\n    plt.xlabel(col)\n    plt.ylabel('Frequency')\n    plt.show()\n\n# 3.2 KDE Plot\nfor col in num_cols:\n    plt.figure(figsize=(6,4))\n    sns.kdeplot(df[col], fill=True)\n    plt.title(f'KDE Plot of {col}')\n    plt.show()\n\n# 3.3 Box Plot (Outliers)\nplt.figure(figsize=(14,6))\ndf[num_cols].boxplot(rot=90)\nplt.title('Boxplot of Numerical Features')\nplt.show()\n\n# -------------------------------\n# 4. CATEGORICAL PLOTS\n# -------------------------------\n\n# 4.1 Count Plot\nfor col in cat_cols:\n    plt.figure(figsize=(6,4))\n    sns.countplot(x=df[col])\n    plt.title(f'Count Plot of {col}')\n    plt.xticks(rotation=45)\n    plt.show()\n\n# 4.2 Bar Plot (Mean of target vs category)\n# Example assumes target column exists\n# Replace 'target' with your label\n\nif 'target' in df.columns:\n    for col in cat_cols:\n        plt.figure(figsize=(6,4))\n        sns.barplot(x=df[col], y=df['target'])\n        plt.title(f'{col} vs Target')\n        plt.xticks(rotation=45)\n        plt.show()\n\n# -------------------------------\n# 5. BIVARIATE PLOTS\n# -------------------------------\n\n# 5.1 Scatter Plot\nif len(num_cols) >= 2:\n    plt.figure(figsize=(6,5))\n    plt.scatter(df[num_cols[0]], df[num_cols[1]])\n    plt.xlabel(num_cols[0])\n    plt.ylabel(num_cols[1])\n    plt.title('Scatter Plot')\n    plt.show()\n\n# 5.2 Scatter with Hue (Target)\nif 'target' in df.columns and len(num_cols) >= 2:\n    plt.figure(figsize=(6,5))\n    sns.scatterplot(x=df[num_cols[0]], y=df[num_cols[1]], hue=df['target'])\n    plt.title('Scatter Plot with Target Hue')\n    plt.show()\n\n# 5.3 Line Plot (Time Series)\n# Use when one column represents time/index\n\n# df.sort_values('date_column', inplace=True)\n# plt.plot(df['date_column'], df['value_column'])\n# plt.title('Line Plot (Time Series)')\n# plt.show()\n\n# -------------------------------\n# 6. MULTIVARIATE PLOTS\n# -------------------------------\n\n# 6.1 Pair Plot\nsns.pairplot(df[num_cols])\nplt.show()\n\n# 6.2 Heatmap (Correlation Matrix)\nplt.figure(figsize=(10,8))\ncorr = df[num_cols].corr()\nsns.heatmap(corr, annot=True, cmap='coolwarm')\nplt.title('Correlation Heatmap')\nplt.show()\n\n# -------------------------------\n# 7. ADVANCED / SPECIAL PLOTS\n# -------------------------------\n\n# 7.1 Violin Plot\nif 'target' in df.columns and len(num_cols) > 0:\n    plt.figure(figsize=(6,4))\n    sns.violinplot(x=df['target'], y=df[num_cols[0]])\n    plt.title('Violin Plot')\n    plt.show()\n\n# 7.2 Strip Plot\nif 'target' in df.columns and len(num_cols) > 0:\n    plt.figure(figsize=(6,4))\n    sns.stripplot(x=df['target'], y=df[num_cols[0]])\n    plt.title('Strip Plot')\n    plt.show()\n\n# 7.3 ECDF Plot\nfor col in num_cols:\n    plt.figure(figsize=(6,4))\n    sns.ecdfplot(df[col])\n    plt.title(f'ECDF of {col}')\n    plt.show()\n\n# -------------------------------\n# 8. DIMENSIONALITY REDUCTION VISUALS\n# -------------------------------\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\n\n# PCA (2D)\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(df[num_cols].dropna())\n\nplt.figure(figsize=(6,5))\nplt.scatter(X_pca[:,0], X_pca[:,1])\nplt.title('PCA Projection (2D)')\nplt.show()\n\n# t-SNE (for visualization)\ntsne = TSNE(n_components=2, perplexity=30, random_state=42)\nX_tsne = tsne.fit_transform(df[num_cols].dropna())\n\nplt.figure(figsize=(6,5))\nplt.scatter(X_tsne[:,0], X_tsne[:,1])\nplt.title('t-SNE Visualization')\nplt.show()\n\n# =====================================================\n# END OF EDA PLOTTING MEGA TEMPLATE\n# =====================================================\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}